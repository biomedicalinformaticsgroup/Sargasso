#!/usr/bin/env bash

set -o nounset
set -o errexit
set -o xtrace

. common_functions

SAMPLES=$1
INPUT_DIR=$2
OUTPUT_DIR=$3
THREADS_PRE_SAMPLE=$4 #How many chunks to split the read files into
MISMATCH_THRESHOLD=$5
MINMATCH_THRESHOLD=$6
MULTIMAP_THRESHOLD=$7
REJECT_MULTIMAPS=$8
TOTAL_THREADS=$9
SPECIES=( "${@:10}" )

NUM_SPECIES=${#SPECIES[@]}



THREAD_USING=0
MEM_USING=0




##### FUNCTIONS

function get_block_file() {
    SAMPLE=$1
    SPECIES=$2
    INDEX=$3

    sep="___"
    echo ${BLOCK_DIR}/"${SAMPLE}${sep}${SPECIES}${sep}"BLOCK"${sep}${INDEX}".bam
}

function get_per_thread_filtered_file() {
    SAMPLE=$1
    SPECIES=$2
    INDEX=$3

    echo "${OUTPUT_DIR}"/"${SAMPLE}"_"${SPECIES}"_"${INDEX}"_filtered.bam
}

function get_output_filtered_file() {
    SAMPLE=$1
    SPECIES=$2

    echo "${OUTPUT_DIR}"/"${SAMPLE}"_"${SPECIES[index]}"_filtered.bam
}

function mf (){
C=$1
echo ${C}
C=2
echo ${C}
}

function create_per_thread_input_files() {
    SAMPLE=$1


    sorted_reads_prefix="${INPUT_DIR}/${SAMPLE}"
    species_bams=()

    index=0
    while [ ${index} -lt ${NUM_SPECIES} ]; do
        species_bams[${index}]="${sorted_reads_prefix}"."${SPECIES[index]}".bam
        index=$((${index} + 1))
    done

    if [ "${THREADS_PRE_SAMPLE}" -eq "1" ]
    then
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            block_file=$(get_block_file ${SAMPLE} ${SPECIES[index]} 1)
            ln -s $(pwd)/${species_bams[index]} ${block_file}
            index=$((${index} + 1))
        done
    else
        read_no=()

        # extract the number of reads for this samples for each species from
        # the mapped reads file
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            read_no[${index}]=$(sambamba view -c -t ${THREADS_PRE_SAMPLE} ${species_bams[index]})
            index=$((${index} + 1))
        done

        # calculate block sizes for larger species
        largest_read_no=0
        largest_read_no_species=""

        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            if [ ${read_no[index]} -gt ${largest_read_no} ]; then
                largest_read_no=${read_no[index]}
                largest_read_no_species=${SPECIES[index]}
            fi
            index=$((${index} + 1))
        done

        block_size=$(( largest_read_no / THREADS_PRE_SAMPLE ))

        # Calculate the line indexes to extract the start/end ids of the blocks
        block_indices=()
        current_index=1

        block_indices["0"]="${current_index}"

        for index in $( seq 1 $(( THREADS_PRE_SAMPLE - 1 )) )
        do
            current_index=$(( current_index + block_size ))
            block_indices["${index}"]="${current_index}"
        done

        block_indices["${THREADS_PRE_SAMPLE}"]="${largest_read_no}"

        # fetch the ids from the largest file & store in array
        sed_command=""

        for i in $(seq $(( THREADS_PRE_SAMPLE + 1)) )
        do
            j=$(( i - 1))
            sed_command="${sed_command} ${block_indices[${j}]}p;"
        done

        block_read_ids=($( sambamba view -t "${THREADS_PRE_SAMPLE}" "${sorted_reads_prefix}"."${largest_read_no_species}".bam | sed -n "${sed_command}" | awk '{print $1}' ))

        # extract blocks for each species
        for i in $(seq "${THREADS_PRE_SAMPLE}" )
        do
            if [ "$i" -ne "${THREADS_PRE_SAMPLE}" ]
            then
                op="<"
            else
                op="<="
            fi

            (
            index=0
            while [ ${index} -lt ${NUM_SPECIES} ]; do
                filter="read_name >= '${block_read_ids[$(( i - 1 ))]}' and read_name ${op} '${block_read_ids[${i}]}'"
                block_file=$(get_block_file ${SAMPLE} ${SPECIES[index]} ${i})
                sambamba view -t 1 --filter "${filter}" ${species_bams[index]} -o ${block_file} -l 1 -h
                index=$((${index} + 1))
            done
            ) &
        done

        wait
    fi
}

function merge_per_thread_filtered_files() {
    SAMPLE=$1

    if [ "${THREADS_PRE_SAMPLE}" -eq "1" ]
    then
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            pt_file=$(get_per_thread_filtered_file ${SAMPLE} ${SPECIES[index]} 0)
            filtered_file=$(get_output_filtered_file ${SAMPLE} ${SPECIES[index]})
            mv ${pt_file} ${filtered_file}
            index=$((${index} + 1))
        done
    else
        # Merge the resultant BAM files produced by the filtering into 1 file
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            pt_files=()
            for i in $(seq 0 1 $((${THREADS_PRE_SAMPLE}-1)));
            do
                pt_files[${i}]=$(get_per_thread_filtered_file ${SAMPLE} ${SPECIES[index]} ${i})
            done

            filtered_file=$(get_output_filtered_file ${SAMPLE} ${SPECIES[index]})
            sambamba merge ${filtered_file} ${pt_files[@]}

            index=$((${index} + 1))
        done
    fi
}

function cleanup_intermediate_files() {
    SAMPLE=$1

    for i in $(seq "${THREADS_PRE_SAMPLE}" )
    do
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            rm $(get_block_file ${SAMPLE} ${SPECIES[index]} ${i})
            index=$((${index} + 1))
        done

        one_less=$((${i} - 1))

        if [ "${THREADS_PRE_SAMPLE}" -ne "1" ]
        then
            index=0
            while [ ${index} -lt ${NUM_SPECIES} ]; do
                rm $(get_per_thread_filtered_file ${SAMPLE} ${SPECIES[index]} ${one_less})
                index=$((${index} + 1))
            done
        fi
    done
}

function calculate_filtering_summary() {
    SAMPLE=$1

    HEADER=""
    TOTALS=()
    TMP_SUMMARY_FILE="${OUTPUT_DIR}"/${SAMPLE}_filtering_result_summary.txt
    OVERALL_SUMMARY_FILE="${OUTPUT_DIR}"/overall_filtering_summary.txt

    while IFS='' read -r line || [[ -n "$line" ]]; 
    do
        if [ -z ${HEADER} ]
        then
                HEADER=($line)
        else
                COUNTS=($line)
                for COUNT in $(seq 0 $((${#COUNTS[@]}-1)) )
                do
                        TOTALS[${COUNT}]=$(( TOTALS[${COUNT}] + COUNTS[${COUNT}] ))
                done
        fi
    done < "${TMP_SUMMARY_FILE}"

    IFS=","

    if [ ! -f ${OVERALL_SUMMARY_FILE} ]
    then
        echo -e "Sample,${HEADER[*]}" > "${OVERALL_SUMMARY_FILE}"
    fi

    echo -e "${gSAMPLE},${TOTALS[*]}" > "${TMP_SUMMARY_FILE}"
    IFS=$' \t\n'
}

function merge_filtering_summary() {
    SAMPLE=$1
    TMP_SUMMARY_FILE="${OUTPUT_DIR}"/${SAMPLE}_filtering_result_summary.txt
    OVERALL_SUMMARY_FILE="${OUTPUT_DIR}"/overall_filtering_summary.txt
    cat ${TMP_SUMMARY_FILE} >> ${OVERALL_SUMMARY_FILE}
    rm ${TMP_SUMMARY_FILE}
}

function do_filter_reads() {
    BLOCK_DIR=${OUTPUT_DIR}/Blocks_${sample}
    mkdir -p $BLOCK_DIR
    create_per_thread_input_files ${sample}
    filter_control ${BLOCK_DIR} ${OUTPUT_DIR} ${sample} ${MISMATCH_THRESHOLD} ${MINMATCH_THRESHOLD} ${MULTIMAP_THRESHOLD} ${REJECT_MULTIMAPS} ${SPECIES[*]}
    merge_per_thread_filtered_files ${sample}
    cleanup_intermediate_files ${sample}
    calculate_filtering_summary ${sample}
}



#####


for sample in ${SAMPLES}; do
    checkBusy
    do_filter_reads &
    #echo "running for ${sample}..." && sleep 2s &
    THREAD_USING=$((${THREAD_USING}+${THREADS_PRE_SAMPLE}))
    MEM_USING=$((${MEM_USING}+30))

done
echo "server busy, ${THREAD_USING} cores/ ${MEM_USING} memory using... waiting for jobs: $(jobs -p) "
wait $(jobs -p)


##merge results
for sample in ${SAMPLES}; do
    merge_filtering_summary ${sample}
done


