#!/usr/bin/env bash

set -o nounset
set -o errexit
set -o xtrace

DATA_TYPE=$1
SAMPLES=$2
INPUT_DIR=$3
OUTPUT_DIR=$4
THREADS=$5 #How many chunks to split the read files into
MISMATCH_THRESHOLD=$6
MINMATCH_THRESHOLD=$7
MULTIMAP_THRESHOLD=$8
REJECT_MULTIMAPS=$9

SPECIES=( "${@:10}" )

NUM_SPECIES=${#SPECIES[@]}
BLOCK_DIR=${OUTPUT_DIR}/Blocks

##### FUNCTIONS

function get_block_file() {
    SAMPLE=$1
    SPECIES=$2
    INDEX=$3

    sep="___"
    echo ${BLOCK_DIR}/"${SAMPLE}${sep}${SPECIES}${sep}"BLOCK"${sep}${INDEX}".bam
}

function get_per_thread_filtered_file() {
    SAMPLE=$1
    SPECIES=$2
    INDEX=$3

    sep="___"
    echo "${OUTPUT_DIR}"/"${SAMPLE}${sep}${SPECIES}${sep}${INDEX}"${sep}filtered.bam
}

function get_output_filtered_file() {
    SAMPLE=$1
    SPECIES=$2
    sep="___"
    echo "${OUTPUT_DIR}"/"${SAMPLE}${sep}${SPECIES[index]}"${sep}filtered.bam
}

function create_per_thread_input_files() {
    SAMPLE=$1

    sorted_reads_prefix="${INPUT_DIR}/${SAMPLE}"
    species_bams=()

    index=0
    while [ ${index} -lt ${NUM_SPECIES} ]; do
        species_bams[${index}]="${sorted_reads_prefix}"."${SPECIES[index]}".bam
        index=$((${index} + 1))
    done

    if [ "${THREADS}" -eq "1" ]
    then
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            block_file=$(get_block_file ${SAMPLE} ${SPECIES[index]} 1)
            ln -s $(pwd)/${species_bams[index]} ${block_file}
            index=$((${index} + 1))
        done
    else
        read_no=()

        # extract the number of reads for this samples for each species from
        # the mapped reads file
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            read_no[${index}]=$(sambamba view -c -t ${THREADS} ${species_bams[index]})
            index=$((${index} + 1))
        done

        # calculate block sizes for larger species
        largest_read_no=0
        largest_read_no_species=""

        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            if [ ${read_no[index]} -gt ${largest_read_no} ]; then
                largest_read_no=${read_no[index]}
                largest_read_no_species=${SPECIES[index]}
            fi
            index=$((${index} + 1))
        done

        block_size=$(( largest_read_no / THREADS ))

        # Calculate the line indexes to extract the start/end ids of the blocks
        block_indices=()
        current_index=1

        block_indices["0"]="${current_index}"

        for index in $( seq 1 $(( THREADS - 1 )) )
        do
            current_index=$(( current_index + block_size ))
            block_indices["${index}"]="${current_index}"
        done

        block_indices["${THREADS}"]="${largest_read_no}"

        # fetch the ids from the largest file & store in array
        sed_command=""

        for i in $(seq $(( THREADS + 1)) )
        do
            j=$(( i - 1))
            sed_command="${sed_command} ${block_indices[${j}]}p;"
        done

        block_read_ids=($( sambamba view -t "${THREADS}" "${sorted_reads_prefix}"."${largest_read_no_species}".bam | sed -n "${sed_command}" | awk '{print $1}' ))

        # extract blocks for each species
        for i in $(seq "${THREADS}" )
        do
            if [ "$i" -ne "${THREADS}" ]
            then
                op="<"
            else
                op="<="
            fi

            (
            index=0
            while [ ${index} -lt ${NUM_SPECIES} ]; do
                filter="read_name >= '${block_read_ids[$(( i - 1 ))]}' and read_name ${op} '${block_read_ids[${i}]}'"
                block_file=$(get_block_file ${SAMPLE} ${SPECIES[index]} ${i})
                sambamba view --filter "${filter}" ${species_bams[index]} -o ${block_file} -l 1 -h
                index=$((${index} + 1))
            done
            ) &
        done

        wait
    fi
}

function merge_per_thread_filtered_files() {
    SAMPLE=$1

    if [ "${THREADS}" -eq "1" ]
    then
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            pt_file=$(get_per_thread_filtered_file ${SAMPLE} ${SPECIES[index]} 0)
            filtered_file=$(get_output_filtered_file ${SAMPLE} ${SPECIES[index]})
            mv ${pt_file} ${filtered_file}
            index=$((${index} + 1))
        done
    else
        # Merge the resultant BAM files produced by the filtering into 1 file
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            pt_files=()
            for i in $(seq 0 1 $((${THREADS}-1)));
            do
                pt_files[${i}]=$(get_per_thread_filtered_file ${SAMPLE} ${SPECIES[index]} ${i})
            done

            filtered_file=$(get_output_filtered_file ${SAMPLE} ${SPECIES[index]})
            sambamba merge -t ${THREADS} ${filtered_file} ${pt_files[@]}

            index=$((${index} + 1))
        done
    fi
}

function cleanup_intermediate_files() {
    SAMPLE=$1

    for i in $(seq "${THREADS}" )
    do
        index=0
        while [ ${index} -lt ${NUM_SPECIES} ]; do
            rm $(get_block_file ${SAMPLE} ${SPECIES[index]} ${i})
            index=$((${index} + 1))
        done

        one_less=$((${i} - 1))

        if [ "${THREADS}" -ne "1" ]
        then
            index=0
            while [ ${index} -lt ${NUM_SPECIES} ]; do
                rm $(get_per_thread_filtered_file ${SAMPLE} ${SPECIES[index]} ${one_less})
                index=$((${index} + 1))
            done
        fi
    done
}

function calculate_filtering_summary() {
    SAMPLE=$1
    FIRST_SAMPLE=$2

    HEADER=""
    TOTALS=()
    TMP_SUMMARY_FILE="${OUTPUT_DIR}"/filtering_result_summary.txt
    OVERALL_SUMMARY_FILE="${OUTPUT_DIR}"/overall_filtering_summary.txt

    while IFS='' read -r line || [[ -n "$line" ]];
    do
        if [ -z ${HEADER} ]
        then
                HEADER=($line)
        else
                COUNTS=($line)
                for COUNT in $(seq 0 $((${#COUNTS[@]}-1)) )
                do
                        TOTALS[${COUNT}]=$(( TOTALS[${COUNT}] + COUNTS[${COUNT}] ))
                done
        fi
    done < "${TMP_SUMMARY_FILE}"

    IFS=","
    if [ ${FIRST_SAMPLE} -eq 1 ]
    then
        echo -e "Sample,${HEADER[*]}" > "${OVERALL_SUMMARY_FILE}"
    fi

    echo -e "${SAMPLE},${TOTALS[*]}" >> "${OVERALL_SUMMARY_FILE}"
    IFS=$' \t\n'

    rm ${TMP_SUMMARY_FILE}
}

#####

mkdir -p $BLOCK_DIR

first_sample=1

for sample in ${SAMPLES}; do
    create_per_thread_input_files ${sample}
    filter_control ${DATA_TYPE} ${BLOCK_DIR} ${OUTPUT_DIR} ${sample} ${MISMATCH_THRESHOLD} ${MINMATCH_THRESHOLD} ${MULTIMAP_THRESHOLD} ${REJECT_MULTIMAPS} ${SPECIES[*]}
    merge_per_thread_filtered_files ${sample}
    cleanup_intermediate_files ${sample}
    calculate_filtering_summary ${sample} ${first_sample}

    if [ ${first_sample} -eq 1 ]
    then
        first_sample=0
    fi
done
